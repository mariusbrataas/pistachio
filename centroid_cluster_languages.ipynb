{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using centroid-based clustering to group languages by similarity\n",
    "In this project I will try to group languages by similarity.\n",
    "\n",
    "I will be using a part of clustering algorithms called \"centroids\". Initially the position of these centroids will depend solely on the position of one datasample each, but as we are fitting the model we'll remove the centroids to which the least amount of languages belong.\n",
    "\n",
    "The training cycle will be:\n",
    "\n",
    "1. Move every centroid to the average position of samples belonging to it (stop when there's no improvement)\n",
    "2. Remove the smallest centroid\n",
    "3. Repeat until there are only a select number of centroids left\n",
    "\n",
    "The data I'll be using for this project was given to me by a friend during a workshop. It consists of txt files with text from wikipedia articles in different languages.\n",
    "\n",
    "\n",
    "## About clustering\n",
    "Clustering is a type of machine learning in which the meaning of the data is found in the relationship between the data samples. This information can be exploited by grouping similar data in different ways, and then look at the collective information in each such group.\n",
    "\n",
    "If you don't have labels for your data this is still a great way to explore it. You may learn more about certain parts of your data by looking at how it relates to other parts of your data, and how strong these connections are.\n",
    "\n",
    "There are many different types of clustering algorithms, and they may accomplish different tasks with different accuracies. The main families in the clustering-algorithm-family-tree are divisive and agglomerative algorithms (respectively dividing data points into groups, or joining data points into groups), both of which contain hierarchical and centroid-based approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's begin by importing the necessary stuff\n",
    "The homemade code for this project will be the \"centroidspace\" class, and the load_languages function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Homemade stuff\n",
    "from clustering.centroidspace import centroidspace\n",
    "from testing_data.load_languages import load_languages\n",
    "\n",
    "# Other stuff\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "For this I've created a function to easily load the data from this repo. The \"raw_articles\" will contain a library with keys for the language of every file we load. A list of all these is then printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Czech\n",
      "Indonesian\n",
      "German\n",
      "Waray-Waray\n",
      "Basque\n",
      "Esperanto\n",
      "Swedish\n",
      "Slovenian\n",
      "Estonian\n",
      "Turkish\n",
      "Finnish\n",
      "Catalan\n",
      "Vietnamese\n",
      "Lithuanian\n",
      "Malay\n",
      "Simple English\n",
      "Uzbek\n",
      "Danish\n",
      "French\n",
      "Galician\n",
      "Cebuano\n",
      "Latin\n",
      "English\n",
      "Minangkabau\n",
      "Hungarian\n",
      "Romanian\n",
      "Spanish\n",
      "Italian\n",
      "Portuguese\n",
      "Dutch\n",
      "Croatian\n",
      "Slovak\n",
      "Polish\n",
      "Norwegian (Nynorsk)\n"
     ]
    }
   ],
   "source": [
    "# Loading text from articles\n",
    "raw_articles = load_languages()\n",
    "\n",
    "# Printing list of languages\n",
    "keys = list(raw_articles.keys())\n",
    "for key in keys: print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figuring out a way to find features in our data\n",
    "The data we have now loaded is contained purely in strings - something that typically won't go well with machine learning algorithms - and thus we'll have to convert it to numbers somehow. We wan't the features of our data to remain intact, and these features should appear independently of how long strings we present to our model.\n",
    "\n",
    "There are probably a bunch of ways to do this, but what I wan't to do here is to check the frequency by which any pair of letters appear in a language. This approach probably won't allow us to find differences between highly similar languages, but it should suffice for this project.\n",
    "\n",
    "In the following function I do just that. It's given a string, loops through the string to count occurences of all pairs of letters. This is then converted to an array and divided by the total number of observations in order to get a ratio for occurences. By doing this division our model should be able to function no matter how long given text samples are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate frequency of character-pair occurences\n",
    "def calculate_frequency(text, length=10000):\n",
    "    # Cleaning up text\n",
    "    text = text.replace('\\n', ' ')[:length].lower()\n",
    "    # Setting up containers\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz '\n",
    "    pairs = {}\n",
    "    for let1 in letters:\n",
    "        for let2 in letters:\n",
    "            pairs[let1+let2] = 0\n",
    "    # Calculating...\n",
    "    total = 0\n",
    "    for i in range(2, len(text)):\n",
    "        pair = text[i-2:i]\n",
    "        if pair in pairs:\n",
    "            pairs[pair] += 1\n",
    "            total += 1\n",
    "    # Returning frequencies\n",
    "    return np.divide([pairs[key] for key in pairs], total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating pair frequencies for all languages\n",
    "X = [calculate_frequency(raw_articles[key], 100000) for key in raw_articles]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model\n",
    "The homemade class for this model is the centroidspace class, which we've already imported. This class utilizes a smaller class called \"centroid\" to handle clusters and their content in a somewhat efficient manner. Having only 34 different classes I'm going to initialize this model with one centroid in the exact location of every data sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating model\n",
    "model = centroidspace(init_positions=X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model\n",
    "As mentioned in the beginning, we will here be doing two things to improve our model. First the model will be fitted to our data for 50 epochs (or until there's no improvement), and then the centroid to which the fewest amount of datapoints belong will be deleted. The process is repeated until only a select number of centroids.\n",
    "\n",
    "Feel free to change the number of centroids. It's interesting to see how languages are grouped when you do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting model\n",
    "model.reductionfit(X, min_centroids=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training finished. Time to see the results!\n",
    "First: Let's look at how the model grouped the languages it was shown during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing and grouping by cluster\n",
    "for i in range(len(keys)):\n",
    "    model.predict(X[i], keys[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Cluster 0 contents:\n",
      "Basque\n",
      "Turkish\n",
      "Uzbek\n",
      "Hungarian\n",
      "\n",
      "Cluster 1 contents:\n",
      "Estonian\n",
      "Finnish\n",
      "Lithuanian\n",
      "\n",
      "Cluster 2 contents:\n",
      "Indonesian\n",
      "Malay\n",
      "Minangkabau\n",
      "\n",
      "Cluster 3 contents:\n",
      "Waray-Waray\n",
      "Vietnamese\n",
      "Cebuano\n",
      "\n",
      "Cluster 4 contents:\n",
      "Catalan\n",
      "French\n",
      "Spanish\n",
      "\n",
      "Cluster 5 contents:\n",
      "Esperanto\n",
      "Galician\n",
      "Latin\n",
      "Romanian\n",
      "Italian\n",
      "Portuguese\n",
      "\n",
      "Cluster 6 contents:\n",
      "German\n",
      "Dutch\n",
      "\n",
      "Cluster 7 contents:\n",
      "Slovenian\n",
      "Croatian\n",
      "\n",
      "Cluster 8 contents:\n",
      "Czech\n",
      "Slovak\n",
      "Polish\n",
      "\n",
      "Cluster 9 contents:\n",
      "Swedish\n",
      "Simple English\n",
      "Danish\n",
      "English\n",
      "Norwegian (Nynorsk)\n"
     ]
    }
   ],
   "source": [
    "# Printing cluster contents\n",
    "for i in range(len(model.centroids)):\n",
    "    print('\\nCluster %s contents:' % i)\n",
    "    for label in model.centroids[i].labels: print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks cool! Our model is able to group all these languages in a way that seems to make sense! I found it interesting that french was put in a cluster with spanish and catalan. I thought it would be closer to english or german."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing with strings it haven't seen yet\n",
    "Let's make sure our model actually figured something out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\"everything gets better with red wine\" is english and the closest centroid also contains\n",
      "- Swedish\n",
      "- Simple English\n",
      "- Danish\n",
      "- English\n",
      "- Norwegian (Nynorsk)\n",
      "\n",
      "\"tutto migliora con il vino rosso\" is italian and the closest centroid also contains\n",
      "- Esperanto\n",
      "- Galician\n",
      "- Latin\n",
      "- Romanian\n",
      "- Italian\n",
      "- Portuguese\n",
      "\n",
      "\"semuanya menjadi lebih baik dengan wain merah\" is malay and the closest centroid also contains\n",
      "- Indonesian\n",
      "- Malay\n",
      "- Minangkabau\n",
      "\n",
      "\"covfefe\" is idiot and the closest centroid also contains\n",
      "- Czech\n",
      "- Slovak\n",
      "- Polish\n"
     ]
    }
   ],
   "source": [
    "# Some testing strings\n",
    "test_lib = {'english': 'everything gets better with red wine',\n",
    "            'italian': 'tutto migliora con il vino rosso',\n",
    "            'malay':   'semuanya menjadi lebih baik dengan wain merah',\n",
    "            'idiot':   'covfefe'}\n",
    "\n",
    "# Running predictions and printing\n",
    "for key in test_lib:\n",
    "    sample = test_lib[key]\n",
    "    pos = calculate_frequency(sample)\n",
    "    closest_centroid = model.predict(pos)\n",
    "    print('\\n\"%s\" is %s and the closest centroid also contains' % (sample, key))\n",
    "    for label in closest_centroid.labels: print('- %s' % label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Everything seems to work pretty well!\n",
    "\n",
    "Please play arround with the number of centroids, test other strings, or write your own versions of this code. I invite anyone who sees this to comment or contribute to this repo.\n",
    "\n",
    "Thanks for reading, and happy hacking!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
